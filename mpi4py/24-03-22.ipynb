{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Computing using `mpi4py`\n",
    "\n",
    "- Genuinely Parallel - when there is inherent parallel (non independent) tasks which require the design of parallel algorithms due to the presence of shared memory and resources, unlike in the case of Embarrasingly/Pleasingly Parallel\n",
    "- More communication between parallel tasks happen, for eg NVLINK, hence more communication overhead\n",
    "- Bandwidth Limitation/Latency Limitation, Latency Limitation can be hidden by doing more work\n",
    "- $T_p = s + pN^{a-1} + c_{\\alpha}(N)$ where the new term is delay due to communication overhead, $c_{\\alpha}(N)$ is not present in the expression of serial processing\n",
    "- Speedup is given by $\\frac{s + pN^{a}}{s + pN^{a-1} + c_{\\alpha}(N)}$ \n",
    "- Domain decomposition is a very essential task and there are many tradeoffs\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recollection of SIMD/MIMD and SPMD/MPMD from previous lectures\n",
    "- Two processes can communicate between each other or we can have network communication\n",
    "- PVM and BSP were used in the past\n",
    "- MPI (Message Interface Passing) is used in the present, it is a standard not a product, it allows communication in the form of messages\n",
    "- Requires MPI libraries to be installed, such as MPICH and OpenMPI and can be hardware specific"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\exists$ CUDA enabled MPI for distributed GPU computing\n",
    "- `mpicc` - the compiler for mpi\n",
    "- `mpirun` - to specify the details of the runtime of specific processor, along with the executable call, processors are specified as rank-0, rank-1, ... rank-N-1 like priority "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development of Parallel Code\n",
    "\n",
    "- Start locally on your own machine, make sure it works, because when you perform on the actual super computer directly it may not work\n",
    "- Performance locally scales well\n",
    "- Prefer switches over routers in the network communication, network properties heavily affect the performance\n",
    "- Test with atleast two smaller PC instead of direct HPC cluster\n",
    "- A cluster has many compute nodes like GPU etc connected with a fast interconnect network, with very low latency (infini-band), there is a network shared file system\n",
    "- Login node is where you login, compile, install, but never run/execute here\n",
    "- Similarly a compute node, never run here too\n",
    "\n",
    "## Guidelines on using a HPC facility\n",
    "- find out about the computational/architecture hardware, CPUs, GPUs and their behaviour, design considerations are different and use cases are different as compared to PC\n",
    "- one needs to know the latencies of the interconnect\n",
    "- MPI flavor and queueing systems\n",
    "- understand the code and choose an appropriate strategy, make test runs to find ideal parameters and make sure to profile and optimize your code, also make production runs before hand.\n",
    "\n",
    "## How to run on a cluster\n",
    "- PBS (Portable Batch System) and SGE(Sun Grid Engine) are queueing systems which will schedule the assigned task, submit the job using `qsub`\n",
    "- SysAd configures Queues and specifies maximum no of cores and maximum runtime\n",
    "\n",
    "## Example Script\n",
    "- `#$` lines in the bash script specifies runtime parameters like in argparse whereas `#` is generic comments\n",
    "\n",
    "## Python Specific Guidelines\n",
    "- install the correct MPI version for python and configure it correctly\n",
    "- `OMP_NUM_THREADS` is an environment variable, which specifies the number of threads that need to be used\n",
    "\n",
    "## `mpi4py`, [Docs](https://mpi4py.readthedocs.io)\n",
    "\n",
    "- install the mpi libraries before `pip install mpi4py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each processor runs the same code \n",
    "- A communicator - `MPI_COMM_WORLD` everyone can talk with everyone else (global)\n",
    "- Every processor has a distinct rank $r \\ge 0 \\in Z$, deterministic specified before hand\n",
    "- same code runs np times, size is the number of processors\n",
    "## Point to Point Communication \n",
    "- use `Send(data, dest, tag)` and `Recv(data, source, tag)`always in pairs\n",
    "- make use tags to differentiate messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sending python objects like dicts is possible, lesser efficient than using numpy arrays, but possible use `send()` here\n",
    "- Blocking: After sending wait for receiver to accept, stay idle till then, if you don't want to block (non-blocking) use `ISend()/Isend()` it is guaranteed to reach as per the MPI standard, but make sure other processor receives\n",
    "- Make a ping-pong test to get effective bandwidth $B_{eff} = \\frac{N}{T_l + \\frac{N}{B}}$, $\\frac{N}{B}$ is actual bandwidth, where $T_l$ is latency "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective Communication (Broadcast/Multicast)\n",
    "\n",
    "- `bcast()` for broadcast of pythonic data, `Bcast()` for numpy, similarly `scatter/Scatter` and `gather/Gather` and `Reduce()` (Reduction from `compyle`), `Allgather` (gather + broadcast)\n",
    "- `Bcast(data, root)`, root is the source of the broadcast, everyone else uses the empty buffer to store the data\n",
    "- `Scatter(sendbuf, recvbuf, root)`, it gathers everything from root and distributes subsets to everyone\n",
    "- `Gather(sendbuf, recvbuf, rank)`, gather is the inverse of scatter, one fellow will get collective of subsets from everyone\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
